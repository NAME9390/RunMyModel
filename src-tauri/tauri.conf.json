{
  "$schema": "https://schema.tauri.app/config/2",
  "productName": "RunMyModel Desktop",
  "version": "1.0.0",
  "identifier": "org.runmymodel.desktop",
  "build": {
    "beforeDevCommand": "npm run dev",
    "devUrl": "http://localhost:1420",
    "beforeBuildCommand": "npm run build",
    "frontendDist": "../dist"
  },
  "app": {
    "windows": [
      {
        "title": "RunMyModel Desktop",
        "width": 1200,
        "height": 800,
        "minWidth": 800,
        "minHeight": 600,
        "resizable": true
      }
    ],
    "security": {
      "csp": null
    }
  },
  "bundle": {
    "active": true,
    "targets": ["app", "dmg", "deb", "appimage", "msi"],
    "icon": [
      "icons/32x32.png",
      "icons/128x128.png",
      "icons/128x128@2x.png",
      "icons/icon.icns",
      "icons/icon.ico"
    ],
    "copyright": "Â© 2024 RunMyModel.org",
    "category": "DeveloperTool",
    "shortDescription": "Run LLMs locally with Ollama",
    "longDescription": "RunMyModel Desktop allows you to run large language models locally using Ollama CLI. Chat with models like LLaMA, Mistral, CodeLlama, and more in a beautiful ChatGPT-style interface.",
    "windows": {
      "certificateThumbprint": null,
      "digestAlgorithm": "sha256",
      "timestampUrl": null,
      "tsp": false
    }
  }
}