# RunMyModel 0.3.0 BETA - Architecture Document

## ğŸ¯ Vision

**RunMyModel 0.3.0 BETA**: A Prompt Architect and Model Runner hybrid desktop application.

Transform from "simple LLM launcher" â†’ "Visual prompt design tool + Local AI execution platform"

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RunMyModel Desktop 0.3.0 BETA              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          C++ Qt6 Frontend (Cross-Platform UI)            â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ Visual Prompt Architect                               â”‚  â”‚
â”‚  â”‚  â€¢ Open WebUI-inspired Design                            â”‚  â”‚
â”‚  â”‚  â€¢ Model Browser & Manager                               â”‚  â”‚
â”‚  â”‚  â€¢ Chat Interface with Streaming                         â”‚  â”‚
â”‚  â”‚  â€¢ Settings & Configuration                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â”‚ REST API / gRPC                          â”‚
â”‚                      â”‚ (HTTP on localhost:5000)                 â”‚
â”‚                      â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚       Python FastAPI Backend (AI Logic)                  â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ llama-cpp-python (GGUF Model Loading)                 â”‚  â”‚
â”‚  â”‚  â€¢ Prompt Template Engine                                â”‚  â”‚
â”‚  â”‚  â€¢ Model Management (list, load, unload)                 â”‚  â”‚
â”‚  â”‚  â€¢ Inference Engine (streaming generation)               â”‚  â”‚
â”‚  â”‚  â€¢ Configuration Management                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              File System Storage                         â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  ~/Documents/rmm/                                        â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ models/          (GGUF files)                       â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ prompts/         (Saved prompt templates)           â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ chats/           (Chat history)                     â”‚  â”‚
â”‚  â”‚  â””â”€â”€ config.json      (App configuration)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§© Component Details

### 1. C++ Qt6 Frontend

**Responsibilities:**
- Modern, responsive UI (Open WebUI-inspired)
- Visual prompt architect (drag-drop blocks)
- Real-time chat interface
- Model browser with GPU filtering
- Download manager with progress
- Settings panel

**Key Files:**
- `src-cpp/main.cpp` - Application entry
- `src-cpp/ui/mainwindow.cpp` - Main window
- `src-cpp/ui/prompt_architect.cpp` - Visual prompt builder
- `src-cpp/ui/chat_window.cpp` - Chat interface
- `src-cpp/api/backend_client.cpp` - Python backend API client

### 2. Python FastAPI Backend

**Responsibilities:**
- GGUF model loading via llama-cpp-python
- Text generation with streaming
- Prompt template management
- Model metadata and caching
- Configuration management

**Key Files:**
```
backend-python/
â”œâ”€â”€ main.py              # FastAPI server entry
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ loader.py        # Model loading/unloading
â”‚   â””â”€â”€ inference.py     # Text generation
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ templates.py     # Prompt template engine
â”‚   â””â”€â”€ architect.py     # Prompt builder logic
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ routes.py        # REST endpoints
â”‚   â””â”€â”€ schemas.py       # Pydantic models
â””â”€â”€ requirements.txt     # Python dependencies
```

### 3. Bridge Layer

**Options:**
1. **REST API** (Recommended for 0.3.0)
   - Python FastAPI backend on `localhost:5000`
   - C++ uses `QNetworkAccessManager` for HTTP requests
   - Simple, debuggable, cross-platform

2. **gRPC** (Future consideration)
   - Higher performance
   - Bidirectional streaming
   - More complex setup

## ğŸ¨ UI Design (Open WebUI-inspired)

### Main Interface Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RunMyModel 0.3.0 BETA                               [_ â–¡ âœ•]       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”¤
â”‚                                                                 â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚                    Prompt Architect                      â”‚   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  ğŸ§± System Prompt                                 â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚     You are a helpful AI assistant specialized   â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚     in [SELECT: coding, writing, analysis]       â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  ğŸ’¬ User Context                                  â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚     Task: [INPUT]                                â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚     Format: [DROPDOWN]                           â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  âš™ï¸ Parameters                                    â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚     Temperature: [SLIDER: 0.7]                   â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚     Max Tokens: [INPUT: 2048]                    â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚
â”‚  â”‚                                                          â”‚   â”‚ â”‚
â”‚  â”‚  [â–¶ Generate]  [ğŸ’¾ Save Template]  [ğŸ”„ Reset]          â”‚   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚                                                                 â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚                    Chat Output                           â”‚   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  ğŸ‘¤ You: Write a Python function...               â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                                    â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  ğŸ¤– Assistant: Here's a Python function that...   â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  ```python                                         â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  def example():                                    â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚      return "Hello, World!"                        â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  ```                                               â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚ â”‚
â”‚  â”‚  [Input...]                              [Send ğŸš€]       â”‚   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚                                                                 â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”˜
```

## ğŸ“‹ API Endpoints (Python Backend)

### Model Management
- `GET /api/models` - List available models
- `GET /api/models/loaded` - List loaded models
- `POST /api/models/load` - Load a model into memory
- `POST /api/models/unload` - Unload a model
- `GET /api/models/{id}/info` - Get model metadata

### Inference
- `POST /api/chat/completions` - Generate text (streaming)
- `POST /api/completions` - Single completion
- `GET /api/chat/history` - Get chat history
- `DELETE /api/chat/history/{id}` - Clear chat

### Prompts
- `GET /api/prompts/templates` - List prompt templates
- `POST /api/prompts/templates` - Save new template
- `GET /api/prompts/templates/{id}` - Get specific template
- `POST /api/prompts/build` - Build prompt from components

### System
- `GET /api/system/info` - GPU, RAM, disk info
- `GET /api/system/health` - Backend health check

## ğŸ”§ Key Features for 0.3.0 BETA

### âœ… Must Have
1. **Visual Prompt Architect**
   - Template-based prompt building
   - Drag-drop components
   - Save/load templates

2. **Real Model Execution**
   - llama-cpp-python integration
   - Streaming text generation
   - Token-by-token display

3. **Model Management**
   - Browse Hugging Face models
   - Download GGUF files
   - Load/unload models dynamically

4. **Modern UI**
   - Open WebUI-inspired design
   - Dark/light theme
   - Responsive layout

5. **Cross-Platform Builds**
   - Linux .AppImage
   - Windows .exe
   - Bundled Python runtime

### ğŸ”„ Nice to Have (Post-0.3.0)
- Multi-modal support (images, audio)
- Fine-tuning interface
- Model comparison tool
- API key management for external services
- Plugin system

## ğŸ“¦ Packaging Strategy

### Linux .AppImage
```bash
# Bundle:
- RunMyModelDesktop (C++ binary)
- Python 3.11 runtime
- backend-python/ (all Python files)
- llama.cpp shared libraries
- Qt6 libraries

# Structure:
AppDir/
â”œâ”€â”€ usr/
â”‚   â”œâ”€â”€ bin/RunMyModelDesktop
â”‚   â”œâ”€â”€ lib/python3.11/
â”‚   â”œâ”€â”€ lib/libQt6*.so
â”‚   â””â”€â”€ share/applications/runmymodel.desktop
â””â”€â”€ AppRun (launch script)
```

### Windows .exe
```bash
# Using PyInstaller + NSIS:
- RunMyModelDesktop.exe (C++ binary)
- python311.dll + embedded Python
- backend-python/ (frozen)
- Qt6 DLLs
- llama.cpp DLLs

# Installer structure:
RunMyModel-0.3.0-Setup.exe
â”œâ”€â”€ RunMyModelDesktop.exe
â”œâ”€â”€ python/
â”œâ”€â”€ backend/
â””â”€â”€ uninstall.exe
```

## ğŸš€ Implementation Roadmap

### Phase 1: Backend Foundation (Week 1)
1. Create Python FastAPI backend
2. Integrate llama-cpp-python
3. Implement basic inference endpoint
4. Test with sample model

### Phase 2: Frontend Redesign (Week 2)
1. Redesign main window layout
2. Create prompt architect UI
3. Implement HTTP client for backend
4. Wire up chat interface

### Phase 3: Integration (Week 3)
1. Connect frontend to backend
2. Implement streaming responses
3. Add model loading UI
4. Test end-to-end flow

### Phase 4: Polish & Package (Week 4)
1. Add prompt templates
2. Improve error handling
3. Create .AppImage build
4. Create .exe build
5. Write user documentation

## ğŸ¯ Success Criteria for 0.3.0 BETA

- âœ… User can visually build prompts
- âœ… User can run GGUF models locally
- âœ… Chat interface streams responses token-by-token
- âœ… Works on Linux and Windows
- âœ… Self-contained executables (no manual Python install)
- âœ… Modern, intuitive UI
- âœ… GPU acceleration works (if available)

## ğŸ“ Notes

- Keep alpha features (model browser, downloads) but integrate into new arch
- Python backend should be a subprocess launched by C++ on startup
- Communication via REST API on localhost (simple, debuggable)
- All AI logic in Python, all UI logic in C++
- Version 0.3.0 is "feature complete beta" - ready for user testing

