# RunMyModel 0.3.0 - Self-Learning LLM Platform

**A local, offline-first AI platform with RAG (Retrieval Augmented Generation) capabilities.**

## âœ¨ Features

- ğŸ¤– **Local LLM Inference** - Run TinyLlama-1.1B locally with llama.cpp
- ğŸ§  **Knowledge Management** - Ingest `.txt` and `.md` files into a vector database
- ğŸ” **RAG System** - Semantic search with FAISS + retrieval-augmented generation
- ğŸ’¾ **Persistent Memory** - SQLite metadata + FAISS vector storage
- ğŸ¨ **Modern Qt6 GUI** - Tabbed interface (Chat / Knowledge / Models)
- âš¡ **Real-time Streaming** - Token-by-token response generation
- ğŸ”’ **100% Offline** - No cloud dependencies, all processing local

## ğŸš€ Quick Start

### Requirements

- **Linux** (tested on Arch/CachyOS, Ubuntu, Fedora)
- **Python 3.9+** with pip
- **Qt6** (Widgets, Core, Gui, Network, Concurrent)
- **GCC/G++** with C++17 support
- **curl** for health checks

### Installation & Run

#### Universal (Any Linux)
```bash
# Clone repository
git clone <your-repo-url>
cd runmymodel-desktop

# Run with ONE command
./run.sh
```

#### Arch Linux / CachyOS (Optimized)
```bash
# Clone repository
git clone <your-repo-url>
cd runmymodel-desktop

# Run with Arch-optimized script
./run_arch.sh
```

**What's the difference?**
- `run.sh` - Universal script, works on any Linux distro
- `run_arch.sh` - Arch-optimized with:
  - Native compiler optimizations (`-march=native -O3`)
  - Better linker flags for performance
  - `systemd-run` for process isolation
  - `fuser` for faster port cleanup
  - Arch-specific package checks

**Both scripts automatically:**
1. âœ… Check all requirements
2. âœ… Set up Python virtual environment
3. âœ… Install dependencies (FAISS, sentence-transformers, FastAPI)
4. âœ… Download embedding model (~90MB, first run only)
5. âœ… Build C++ application
6. âœ… Start knowledge service (port 8001)
7. âœ… Launch GUI application
8. âœ… Clean up on exit

## ğŸ“– Usage

### 1. Chat Tab ğŸ’¬

- Type your message and press Enter or click "Send"
- Toggle "Use RAG" to enable knowledge-enhanced responses
- Real-time streaming of AI responses

### 2. Knowledge Tab ğŸ§ 

**Upload Knowledge:**
- Click "ğŸ“ Upload File"
- Select `.txt` or `.md` files
- Watch as the system chunks, embeds, and indexes your knowledge

**Manage Knowledge:**
- View all ingested sources
- See statistics (chunks, vectors, sources)
- Delete sources you no longer need

**RAG Workflow:**
1. Upload a document (e.g., `python_tutorial.md`)
2. Enable "Use RAG" in Chat tab
3. Ask a question related to the document
4. The AI retrieves relevant chunks and generates an enhanced response!

### 3. Models Tab âš™ï¸

- View current model information
- (Future: Switch models, add API keys)

## ğŸ§ª Testing RAG

A test file is included to try the RAG system:

```bash
# 1. Start the application
./run.sh  # or ./run_arch.sh on Arch

# 2. In the GUI:
#    - Go to "Knowledge" tab
#    - Click "Upload File"
#    - Select "test_knowledge.md"
#    - Wait 5 seconds for ingestion

# 3. Test it:
#    - Go to "Chat" tab
#    - Enable "Use RAG"
#    - Ask: "How do I print text in Python?"
#    - Watch the AI use your knowledge! ğŸ¯
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         C++ Qt6 GUI Application         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Chat   â”‚ â”‚Knowledge â”‚ â”‚ Models  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚ HTTP (port 8001)
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Llama   â”‚    â”‚Knowledgeâ”‚
    â”‚ Engine  â”‚    â”‚ Client  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚llama.cppâ”‚    â”‚ FastAPI â”‚
    â”‚(TinyLlama)    â”‚ Service â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Knowledgeâ”‚
                   â”‚ Manager â”‚
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                   â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚  FAISS  â”‚         â”‚ SQLite  â”‚
         â”‚(vectors)â”‚         â”‚(metadata)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
runmymodel-desktop/
â”œâ”€â”€ run.sh                   # â­ Universal launcher
â”œâ”€â”€ run_arch.sh              # â­ Arch-optimized launcher
â”œâ”€â”€ build.sh                 # Build script
â”œâ”€â”€ src-cpp/                 # C++ source code
â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â”œâ”€â”€ mainwindow.h
â”‚   â”‚   â”œâ”€â”€ llama_engine.h
â”‚   â”‚   â””â”€â”€ knowledge_client.h
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.cpp
â”‚       â”œâ”€â”€ mainwindow.cpp
â”‚       â”œâ”€â”€ llama_engine.cpp
â”‚       â””â”€â”€ knowledge_client.cpp
â”œâ”€â”€ knowledge_service/       # Python FastAPI service
â”‚   â”œâ”€â”€ main.py              # FastAPI server
â”‚   â”œâ”€â”€ knowledge_manager.py # FAISS + SQLite manager
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ models/                  # LLM models (.gguf)
â”‚   â””â”€â”€ tinyllama.gguf
â”œâ”€â”€ memory/                  # Knowledge storage
â”‚   â”œâ”€â”€ faiss_index.bin      # Vector index
â”‚   â””â”€â”€ metadata.db          # SQLite database
â”œâ”€â”€ config/                  # Configuration
â”‚   â””â”€â”€ default_config.json
â”œâ”€â”€ test_knowledge.md        # â­ Test file for RAG
â””â”€â”€ lib/
    â””â”€â”€ llama.cpp/           # llama.cpp library
```

## ğŸ› ï¸ Development

### Manual Build

```bash
# Build only
./build.sh

# Start knowledge service manually
cd knowledge_service
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python3 main.py

# Run GUI (in another terminal)
export LD_LIBRARY_PATH="$(pwd)/build/lib:$(pwd)/lib/llama.cpp/build/bin"
./build/RunMyModelDesktop
```

### Rebuild Application

```bash
# Force rebuild
rm -rf build/
./run.sh  # or ./run_arch.sh
```

### Testing Knowledge Service API

```bash
# Health check
curl http://127.0.0.1:8001/health

# Upload knowledge
curl -X POST http://127.0.0.1:8001/knowledge/ingest \
  -H "Content-Type: application/json" \
  -d '{"text": "Python is a programming language", "source": "test.txt"}'

# Query knowledge
curl -X POST http://127.0.0.1:8001/knowledge/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Python?", "top_k": 3}'
```

## ğŸ› Troubleshooting

**Port 8001 already in use:**
```bash
# Universal
lsof -i:8001
pkill -9 -f "knowledge_service"

# Arch (faster)
fuser -k 8001/tcp
```

**Knowledge service logs:**
```bash
tail -f knowledge_service.log
```

**Build fails:**
```bash
# Check Qt6 installation
pkg-config --modversion Qt6Core  # Universal
pacman -Qi qt6-base              # Arch

# Check GCC
g++ --version

# Rebuild llama.cpp if needed
cd lib/llama.cpp
rm -rf build
mkdir build && cd build
export CMAKE_ROOT=/usr/share/cmake
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)
```

**Model not found:**
```bash
# Download TinyLlama (638MB)
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O models/tinyllama.gguf
```

## ğŸ“Š Technical Details

- **LLM Engine:** llama.cpp with GGUF models
- **Embedding Model:** all-MiniLM-L6-v2 (384-dim, ~90MB)
- **Vector DB:** FAISS (Facebook AI Similarity Search)
- **Metadata DB:** SQLite
- **Web Framework:** FastAPI (Python)
- **GUI Framework:** Qt6 (C++)
- **Chunking:** tiktoken (512 tokens, 50 overlap)
- **Retrieval:** Cosine similarity (top-5 by default)

## ğŸ”® Future Plans

- [ ] Multiple model support
- [ ] API key integration (OpenAI, HuggingFace)
- [ ] Session persistence / chat history
- [ ] Fine-tuning capabilities
- [ ] Knowledge visualization (topic clusters)
- [ ] Export/import knowledge base
- [ ] Windows support
- [ ] AppImage packaging

## ğŸ“„ License

MPL 2.0 - See LICENSE file

## ğŸ¤ Contributing

See CONTRIBUTING.md for guidelines.

---

**Built with â¤ï¸ for the local AI community**

**Just run `./run.sh` (or `./run_arch.sh` on Arch) and you're ready to go!** ğŸš€
